---
title: "R Notebook"
output: html_notebook
---
## Read library
```{r}
library('readxl')
df=read_excel('C:\\Users\\Rushi\\Downloads\\IPYNB\\adult.xlsx')
df
```
## Replace ? categorical values in columns with their column modes
```{r}
getmode <- function(v){
  v=v[nchar(as.character(v))>0]
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], function(x) ifelse(x=="?", getmode(x), x))
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)
df
```
```{r}
library(ggplot2)
# histogram of age by income group
ggplot(df) + aes(x=as.numeric(age), group=income, fill=income) + 
  geom_histogram(binwidth=1, color='black')
```
```{r}
library(esquisse)
esquisser(df,viewer='browser')
```
```{r}
ggplot(df) +
  aes(x = age, fill = income, colour = income) +
  geom_histogram(bins = 30L) +
  scale_fill_hue(direction = 1) +
  scale_color_hue(direction = 1) +
  labs(
    x = "age",
    y = "count",
    title = "histogram of age by gender group",
    fill = "income",
    color = "income"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5))
```

```{r}
ggplot(df) +
  aes(
    x = workclass,
    y = hours.per.week,
    fill = income,
    colour = income
  ) +
  geom_jitter(size = 1.5) +
  scale_fill_hue(direction = 1) +
  scale_color_hue(direction = 1) +
  labs(
    x = "age",
    y = "count",
    title = "histogram of age by gender group",
    fill = "income",
    color = "income"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", hjust = 0.5))
```


```{r}
summary(df$workclass)
```
## Model Fitting
m1 <- glm(income ~ ., data = training_set, family = binomial('logit'))
summary(m1)
```{r}
# create a dataframe
sz <- round(.8 * dim(df)[1])  # training set size
training_set <- df[1:sz,]
testing_set <- df[-(1:sz),]
```
## Next, a classification and regression tree (CART) is grown on the training set. The CART model is obtained by recursively partitioning the data space and fitting a simple prediction model within each partition. Although it is generally named CART, the tree grown in this case is actually a classification tree.
```{r}
library(rpart)
tree2 <- rpart(income ~ ., data = training_set, method = 'class', cp = 1e-3)
tree2.pred.prob <- predict(tree2, newdata = testing_set, type = 'prob')
tree2.pred <- predict(tree2, newdata = testing_set, type = 'class')
# confusion matrix 
tb2 <- table(tree2.pred, testing_set$income)
tb2
```
The prediction result of CART has an accuracy of **82.94%,** and a missclassification rate of **17.06%**.

## Random forest (RF) is another powerful machine learning tool. It improves predictive accuracy by generating a large number of bootstrapped trees. Final predicted outcome is attained by combining the results across all of the trees.
```{r}
library(randomForest)
rf3 <- randomForest(income ~ ., data = training_set, ntree = 1000)
rf3.pred.prob <- predict(rf3, newdata = testing_set, type = 'prob')
rf3.pred <- predict(rf3, newdata = testing_set, type = 'class')
# confusion matrix 
tb3 <- table(rf3.pred, testing_set$income)
tb3
```
The prediction result of RF has an accuracy of **85.36%,** and a misclassification rate of **14.63%.**

## Last, I use support vector machine (SVM) to predict the income level. SVM is a discriminative classifier that constructs a hyperplane in a high dimensional space used for classification.
```{r}
library(kernlab)
svm4 <- ksvm(income ~ ., data = training_set)
svm4.pred.prob <- predict(svm4, newdata = testing_set, type = 'decision')
svm4.pred <- predict(svm4, newdata = testing_set, type = 'response')
# confusion matrix 
tb4 <- table(svm4.pred, testing_set$income)
tb4
```
The prediction result of RF has an accuracy of **85.21%,** and a misclassification rate of **14.78%.**
## Conclusion

- ROC curve is a plot of true positive rate against false positive rate under all threshold values. Confusion matrix is a measurement of overall prediction accuracy. Since the majority of observations in the data set has income less than $50,000 a year, sensitivity and specificity contribute to the overall accuracy by different weights. 

- CART has the lowest AUC. Machine learning is no denying a powerful, but it should not be considered as a substitute of traditional statistical modeling.

